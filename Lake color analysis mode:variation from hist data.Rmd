---
title: "Lake color analysis mode/variation"
author: "Xiao Yang"
date: "5/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(sf) 

chroma <- function(R, G, B) {
  require(colorscience)

  Xi <- 2.7689*R + 1.7517*G + 1.1302*B
  Yi <- 1.0000*R + 4.5907*G + 0.0601*B
  Zi <- 0.0565*G + 5.5943*B

  x <-  Xi / (Xi + Yi +  Zi)
  y <-  Yi / (Xi + Yi +  Zi)
  z <-  Zi / (Xi + Yi +  Zi)

  alpha <- atan2( (x - 0.33), (y - 0.33)) * 180/pi

  cie <- colorscience::cccie31 %>%
    mutate(a = atan2( (x - 0.33), (y - 0.33)) * 180/pi) %>%
    dplyr::filter(wlnm <= 700) %>%
    dplyr::filter(wlnm >= 380) %>%
    select(a, wlnm) %>%
    arrange(a)

  wl <- cie[as.vector(sapply(alpha,function(x) which.min(abs(x - cie$a)))), 'wlnm']

  return(wl)
}
Adjust = function (myColor, Factor, Gamma, IntensityMax = 1) {
    if (length(myColor) == 1) {
        if (myColor == 0) {
          return(0)}
        else {
          return(IntensityMax * (myColor * Factor)^Gamma)
          }
    }
    else {
        w <- which(myColor != 0)
        myColor[w] <- IntensityMax * (myColor[w] * Factor[w])^Gamma
        return(myColor)
    }
}
heuristic.wlnm2RGB_modified = function(wavelength, Gamma = 0.8, IntensityMax = 1) {
    if (IntensityMax < 0) 
        IntensityMax <- 0
    if (IntensityMax > 1) 
        IntensityMax <- 1
    wavelength <- round(wavelength, digits = 0)
    Red <- Green <- Blue <- factorL <- rep(0, length(wavelength))
    w <- which(wavelength > 380 & wavelength <= 440)
    if (length(w) > 0) {
        Red[w] <- -(wavelength[w] - 440)/(440 - 380)
        Green[w] <- 0
        Blue[w] <- 1
    }
    w <- which(wavelength > 440 & wavelength <= 490)
    if (length(w) > 0) {
        Red[w] <- 0
        Green[w] <- (wavelength[w] - 440)/(490 - 440)
        Blue[w] <- 1
    }
    w <- which(wavelength > 490 & wavelength <= 510)
    if (length(w) > 0) {
        Red[w] <- 0
        Green[w] <- 1
        Blue[w] <- -(wavelength[w] - 510)/(510 - 490)
    }
    w <- which(wavelength > 510 & wavelength <= 580)
    if (length(w) > 0) {
        Red[w] <- (wavelength[w] - 510)/(580 - 510)
        Green[w] <- 1
        Blue[w] <- 0
    }
    w <- which(wavelength > 580 & wavelength <= 645)
    if (length(w) > 0) {
        Red[w] <- 1
        Green[w] <- -(wavelength[w] - 645)/(645 - 580)
        Blue[w] <- 0
    }
    w <- which(wavelength > 645 & wavelength <= 780)
    if (length(w) > 0) {
        Red[w] <- 1
        Green[w] <- 0
        Blue[w] <- 0
    }
    w <- which(wavelength > 380 & wavelength <= 420)
    if (length(w) > 0) 
        factorL[w] <- 0.3 + 0.7 * (wavelength[w] - 380)/(420 - 
            380)
    w <- which(wavelength > 420 & wavelength <= 700)
    if (length(w) > 0) 
        factorL[w] <- 1
    w <- which(wavelength > 700 & wavelength <= 780)
    if (length(w) > 0) 
        factorL[w] <- 0.3 + 0.7 * (780 - wavelength[w])/(780 - 
            700)
    R <- Adjust(Red, factorL, Gamma, IntensityMax)
    G <- Adjust(Green, factorL, Gamma, IntensityMax)
    B <- Adjust(Blue, factorL, Gamma, IntensityMax)
    
    output = as_tibble(list(R = R, G = G, B = B)) %>% 
      transmute(color = rgb(R, G, B, maxColorValue = 1))
    
    return(output)
}
unpackHist = function(histString, colNames) {
  # take histString as input (output from ee.Reducer.fixedHistogram() on GEE)
  # return a 2-column tibble: first column the value of the left break and second column is the count in the bin
  output = (histString %>% 
              str_remove_all(pattern = "\\[|\\]") %>% 
              str_split(pattern = ","))[[1]] %>% 
    as.numeric() %>% 
    matrix(ncol = 2, byrow = T) %>% 
    as_tibble() %>% 
    filter(V2 != 0) %>% 
    mutate(V2 = as.integer(V2))
  
  names(output) = colNames
  
  return(output)
}
```

## Overview
This script analyze lake color mode and standard deviation calculated from most recent 7 years of Landsat 8 data (SR, Collection 1 Tier 1, cloud <= 25, 05-30-2013 to 05-31-2020)

```{r}
lakes = st_read("outputs/lake_samples_jida_05172020.shp")
lakes_meta = lakes %>% mutate(cd = st_centroid(geometry), clon = st_coordinates(cd)[, 1], clat = st_coordinates(cd)[, 2], area = lwgeom::st_geod_area(geometry)) %>% select(id, clon, clat, area) %>% st_drop_geometry()
remove(lakes)
```

## import data

```{r}
require(stringr)
require(diptest)
## load lake data 

version = "6c9bad6114663fb69424dc874a840afd"

## why there are so many lakes having NA values?
i = 1
test = read_csv(dir(path = "data/6c9bad6114663fb69424dc874a840afd/", full.names = T, pattern = "*.csv")[i])

nFiles = length(dir(path = "data/6c9bad6114663fb69424dc874a840afd/", full.names = T, pattern = "*.csv"))


for (i in 1:nFiles) {
  
  print(paste0("File ", i, " started..."))
  
  datIn = read_csv(dir(path = "data/6c9bad6114663fb69424dc874a840afd", full.names = T, pattern = "*.csv")[i], col_types = "cnnncnnicnnicnnnn") %>% 
    na.omit()
  
  #
  datDw = datIn %>% filter(dw_count >= 10) %>% select(id, histgram = dw_histgram) %>% mutate(source = "dw")
  datDwLehmann = datIn %>% filter(dw_count >= 10) %>% select(id, histgram = dwLehmann_histgram) %>% mutate(source = "dwLehmann")
  datHue = datIn %>% filter(dw_count >= 10) %>% select(id, histgram = hue_histgram) %>% mutate(source = "hue")
  
  histMerged = bind_rows(datDw, datDwLehmann, datHue)
  
  thisHistUnpacked = histMerged %>% 
    group_by(id, source) %>% 
    do({
      dat = .
      unpackHist(dat$histgram[1], colNames = c("metric", "count"))
    }) %>% 
    ungroup()
  
  sourceIntervals = tibble(source = c("dw", "dwLehmann", "hue"), halfInterval = c(5, 5, 1 / 64) / 2)
  
  thisHistModeTest = thisHistUnpacked %>% 
    left_join(sourceIntervals, by = "source") %>% 
    mutate(metricMid = metric + halfInterval) %>% 
    group_by(id, source) %>% 
    do({
      dat = .
      unpackedCount = dat %>% 
        group_by(metric) %>% 
        do({
          datm = .
          tibble(metricUnpacked = rep(x = datm$metricMid[1], times = datm$count[1]))
          }) %>% 
        ungroup()
      
      values = unpackedCount %>% pull(metricUnpacked) # reconstructed values
      dip = values %>% dip.test()
      d = values %>% density()
      mode = d$x[which.max(d$y)][1]
      
      dip[1:3] %>% as_tibble() %>% mutate(mode_density = mode, bw = d$bw)
    }) %>% 
    ungroup()
  
  thisDatOut = datIn %>% 
    select(-hue_histgram, -dw_histgram, -dwLehmann_histgram) %>% 
    left_join(thisHistModeTest %>% select(id, source, p.value) %>% spread(key = source, value = p.value) %>% rename(dw.pv = dw, dwLehmann.pv = dwLehmann, hue.pv = hue), by = "id") %>% 
    left_join(thisHistModeTest %>% select(id, source, mode_density) %>% spread(key = source, value = mode_density) %>% rename(dw.mode = dw, dwLehmann.mode = dwLehmann, hue.mode = hue), by = "id")
  
  save(thisHistModeTest, thisDatOut, file = paste0("outputs/histUnpacked_", i, ".RData"))
  
  if (i == 1) {
    histModeTest = thisHistModeTest
    datOut = thisDatOut
    histUnpacked = thisHistUnpacked
  }
  if (i != 1) {
    histModeTest = histModeTest %>% bind_rows(thisHistModeTest)
    datOut = datOut %>% bind_rows(thisDatOut)
    histUnpacked = histUnpacked %>% bind_rows(thisHistUnpacked)
  }
}

save(histModeTest, datOut, histUnpacked, file = paste0("outputs/histUnpacked_", version, ".RData"))

load(paste0("outputs/histUnpacked_", version, ".RData"), verbose = T)

dat = datOut
remove(datOut)

save(dat, file = paste0("outputs/lake_color_data_", version, ".RData"))

load(paste0("outputs/lake_color_data_", version, ".RData"), verbose = T)
write_csv(dat %>% na.omit(), path = paste0("outputs/lake_color_data_", version, ".csv"))

## plot some results

sampledIds = histModeTest %>% 
  filter(source == "dw"
         # nobs >= 25
         ) %>% 
  mutate(unimodal = p.value >= 0.05) %>% 
  group_by(unimodal) %>% 
  sample_n(1) %>% 
  ungroup()

histUnpacked %>% 
  right_join(sampledIds, by = c("id", "source")) %>% 
  ggplot() +
  geom_bar(aes(x = metric, y = count, fill = id), stat = "identity", width = 5, color = "white") +
  # geom_line(aes(x = metric, y = count, color = id)) +
  geom_vline(aes(xintercept = mode_density), color = "black") +
  facet_wrap(~unimodal, ncol = 1, scales = "free_y")
```

<!-- ## Mode color directly calculated on GEE, dominant wavelength averaged within a 60 meter buffer around the deepest points. -->
<!-- ```{r} -->
<!-- version = "fae69310b5b0b01aebf7bd475c5a4f2a" -->

<!-- dat = dir("data/fae69310b5b0b01aebf7bd475c5a4f2a", pattern = "*.csv", full.names = T) %>%  -->
<!--   map(read_csv, col_types = "cnnnnnnnnnnnnn") %>%  -->
<!--   reduce(bind_rows) -->

<!-- print("How many lakes have NA values for color?") -->
<!-- dat %>% filter(is.na(hue_mode)) %>% nrow() -->

<!-- dat = dat %>% na.omit() %>% distinct() -->

<!-- save(dat, file = "outputs/lake_color_data_fae69310b5b0b01aebf7bd475c5a4f2a.RData") -->
<!-- write_csv(dat, path = "outputs/lake_color_data_fae69310b5b0b01aebf7bd475c5a4f2a.csv") -->
<!-- ``` -->

## decide which mode and dw method to use

```{r}
version = "6c9bad6114663fb69424dc874a840afd"
load(paste0("outputs/lake_color_data_", version, ".RData"), verbose = T)

## here we used the dominant wavelength algorithm similar to that used to calculate river color
dat = dat %>% select(id, distance, mode_dw = dw.mode, std_dw = dw_stdDev, nobs = dw_count, waterOcc, dw.pv) %>% distinct() %>% na.omit()
```

## attach lake meta data

```{r}
## add lake area, centroid lat and lon
lakes = st_read("outputs/lake_samples_jida_05172020.shp")
lakes_meta = lakes %>% mutate(cd = st_centroid(geometry), clon = st_coordinates(cd)[, 1], clat = st_coordinates(cd)[, 2], area = lwgeom::st_geod_area(geometry)) %>% select(id, clon, clat, area) %>% st_drop_geometry()

dat = dat %>% left_join(lakes_meta, by = "id")
remove(lakes_meta, lakes)

# attach Depth and lake type
datLakeMeta = read_csv("outputs/lake_meta_v2.csv", col_types = "nccnnc")
dat = dat %>% 
  inner_join(datLakeMeta %>% select(id, depth = depthMean, class), by = "id") %>% 
  mutate(class = factor(class, levels = c(1:5, NA), labels = c("Endorheic Saline", "Endorheic Fresh", "Exorheic Saline", "Exorheic Fresh", "Lagoon", "Reservoir"), ordered = T, exclude = NULL))
remove(datLakeMeta)

# attach lon and lat for dp
lakeSampleDp = st_read("outputs/lakeSample_Dp.geojson")
lakeDpCoords = lakeSampleDp %>% filter(type == "dp") %>% mutate(dlon = st_coordinates(geometry)[, 1], dlat = st_coordinates(geometry)[, 2]) %>% st_drop_geometry() %>% select(id, dlon, dlat) %>% distinct() %>% as_tibble()
dat = dat %>% left_join(lakeDpCoords, by = "id")

save(dat, file = paste0("outputs/lake_color_mergedMeta_", version, ".RData"))

load(paste0("outputs/lake_color_mergedMeta_", version, ".RData"), verbose = T)
```

## Distribution of color

```{r}
print("How many lakes have hue_count less than 10?")
dat %>% filter(nobs < 10) %>% select(id) %>% distinct() %>% nrow()

print("How many lakes have water occurrence less than 90?")
dat %>% filter(waterOcc < 90) %>% nrow()

datFil = dat %>% 
  filter(nobs >= 10,
         waterOcc >= 90)

datFil %>% nrow()


## histogram of color
dw_distribution = datFil %>% 
  ggplot() +
  geom_histogram(aes(x = mode_dw), binwidth = 5, color = "white") + labs(x = "Mode dominant wavelength (nm)", y = "No. of lakes") + scale_x_continuous(limits = c(470, 600), expand = c(0, 0))

dw_distribution

dw_distribution %>% ggsave(filename = paste0("figs/dw_distribution_", version, ".png"), width = 4, height = 3)

ecdf_dw = tibble(dw = seq(min(datFil$mode_dw), max(datFil$mode_dw), length.out = 500)) %>% 
  group_by(dw) %>% 
  do({
    temp = .
    datFil %>% filter(mode_dw <= temp$dw[1]) %>% summarise(cumn = n(), cumArea = sum(area) / 1000000)
  }) %>% 
  ungroup()

maxValues = ecdf_dw %>% summarise(nlakes = max(cumn), totalArea = max(cumArea))

ecdf_dw %>% 
  mutate(cumnPercent = cumn / maxValues$nlakes[1], cumAreaPercent = cumArea / maxValues$totalArea[1]) %>% 
  gather(key = "metric", value = "value", c(cumnPercent, cumAreaPercent)) %>% 
  mutate(metric = factor(metric, levels = c("cumnPercent", "cumAreaPercent"), labels = c("No. of lakes", "Surface area"))) %>% 
  ggplot() +
  geom_line(aes(x = dw, y = value, color = metric)) +
  scale_x_continuous(limits = c(450, 600)) +
  scale_y_continuous(labels = scales::percent_format(1))
```

<!-- ### Summary stats -->

<!-- ```{r} -->
<!-- ## density plot -->
<!-- # calc_hue_density = function(hue) { -->
<!-- #    -->
<!-- #   d = density(hue, n = 512) -->
<!-- #    -->
<!-- #   hue_density = tibble(hue = d$x, density = d$y) %>%  -->
<!-- #     filter(hue >= 0, hue <= 1) %>%  -->
<!-- #     mutate(rgb_color = hsv(h = hue, s = 1, v = 1)) -->
<!-- #    -->
<!-- #   return(hue_density) -->
<!-- # } -->
<!-- calc_dw_density = function(dw) { -->

<!--   d = density(dw, n = 512) -->

<!--   # dw_density = tibble(dw = d$x, density = d$y) %>% -->
<!--   #   group_by(dw, density) %>% -->
<!--   #   do({ -->
<!--   #     temp = . -->
<!--   #     heuristic.wlnm2RGB(temp$dw[1]) -->
<!--   #     }) %>% -->
<!--   #   ungroup() %>% -->
<!--   #   rename(rgb_color = color) -->

<!--   dw_density = tibble(dw = d$x, density = d$y) %>% -->
<!--     bind_cols(heuristic.wlnm2RGB_modified(d$x, Gamma = 1.5)) %>% -->
<!--     rename(rgb_color = color) -->

<!--   return(dw_density) -->
<!-- } -->

<!-- # hue_density = calc_hue_density(dat$hue_mode) -->
<!-- dw_density = calc_dw_density(datFil$mode_dw) -->

<!-- dw_density %>% filter(dw <= 530) %>% arrange(desc(density)) -->
<!-- dw_density %>% filter(dw >= 530) %>% arrange(desc(density)) -->

<!-- combined = dw_density %>% rename(value = dw) %>% mutate(metric = "Mode(Dominant wavelength) (nm)") -->

<!-- all_lake_color = combined %>%  -->
<!--   ggplot(aes(value, density)) +  -->
<!--   geom_segment(aes(xend = value, yend = 0, color = rgb_color), alpha = 1) +  -->
<!--   geom_line(lwd = 1) +  -->
<!--   scale_color_identity() + -->
<!--   # scale_x_continuous(limits = c(0, 1)) + -->
<!--   labs( -->
<!--     title = "Distribution of most frequent lake colors" -->
<!--   ) + -->
<!--   theme( -->
<!--     panel.background = element_blank(), -->
<!--     axis.title.y = element_blank(), -->
<!--     axis.text.y = element_blank(), -->
<!--     axis.ticks.y = element_blank() -->
<!--   ) + -->
<!--   facet_wrap(~metric, scales = "free", ncol = 1) -->

<!-- all_lake_color -->

<!-- all_lake_color %>% ggsave(filename = paste0("figs/all_lake_color_dw_", version, ".png"), width = 6, height = 6) -->
<!-- ``` -->

### Maps
mapping actual lake polygons is very slow even with only 1000 lakes
```{r}
lakes = st_read("outputs/lake_samples_jida_05172020.shp") %>% select(id)

datFil_sf = datFil %>% 
  bind_cols(tibble(fillColor = heuristic.wlnm2RGB_modified(datFil$mode_dw)$color)) %>% 
  left_join(lakes, by = "id") %>% 
  st_as_sf(crs = 4326) 
  
st_crs(datFil_sf) = 4326
## this function calculate x and y limits under given new crs
calculate_map_bounds = function(minlat, maxlat, minlon, maxlon, crs) {
  line1 = st_linestring(x = matrix(c(minlon, minlat, maxlon, maxlat), byrow = T, nrow = 2), dim = "XY")
  line2 = st_linestring(x = matrix(c(minlon, maxlat, maxlon, minlat), byrow = T, nrow = 2), dim = "XY")
  output = st_as_sfc(list(line1, line2), crs = 4326) %>% st_transform(crs) %>% st_bbox
  
  return(output)
}

xylim = calculate_map_bounds(minlat = -50, maxlat = 80, minlon = -180, maxlon = 180, crs = 4326) ## say you want to plot 40N and above with polar projection EPSG:3995

# rnaturalearthdata is a package in R that contains world map data, good for map background
world = rnaturalearthdata::countries50 %>% st_as_sf %>% 
  st_transform(crs = 4326)

map_mode = datFil_sf %>% ## replace lakes as your own input of the shp file. you can import shp file using st_read()
  st_transform(crs = 4326) %>% 
  # mutate(plotColor = hsv(mode_hu, s = 1, v = 0.8)) %>% 
  sample_n(1000) %>%
  ggplot() +
  geom_sf(data = world, fill = "black", color = NA) +
  geom_sf(aes(fill = fillColor), color = NA) +
  coord_sf(crs = st_crs(4326),
           xlim = c(xylim[1], xylim[3]),
           ylim = c(xylim[2], xylim[4]),
           expand = T) +
  labs(fill = "") +
  scale_fill_identity() +
  theme(panel.grid.major = element_line(color = "white", size = 0.5),
        axis.text.y = element_blank(),
        axis.text.x = element_blank(),
        line = element_blank(),
        rect = element_blank(),
        text = element_text(size = 14),
        panel.grid = element_blank(),
        legend.position = c(0.125, 0.15),
        axis.title = element_blank())

map_mode

map_mode %>% ggsave(filename = "figs/lake_mode_color_map.png", width = 10, height = 5, dpi = 300)


map_stdDev = datFil_sf %>% ## replace lakes as your own input of the shp file. you can import shp file using st_read()
  st_transform(crs = 54030) %>% 
  # sample_n(2000) %>%
  ggplot() +
  geom_sf(data = world, fill = "black", color = NA) +
  geom_sf(aes(color = hue_stdDev), size = .1, alpha = 0.5) +
  coord_sf(crs = st_crs(54030),
           xlim = c(xylim[1], xylim[3]),
           ylim = c(xylim[2], xylim[4]),
           expand = T) +
  labs(size = "Number of instances",
       color = "") +
  scale_color_viridis_c(begin = 0.2) +
  theme(panel.grid.major = element_line(color = "white", size = 0.5),
        axis.text.y = element_blank(),
        axis.text.x = element_blank(),
        line = element_blank(),
        rect = element_blank(),
        text = element_text(size = 14),
        panel.grid = element_blank(),
        legend.position = c(0.125, 0.15),
        axis.title = element_blank())

# map_stdDev

map_stdDev %>% 
  ggsave(filename = "figs/lake_color_stdDev_map.png", width = 10, height = 5, dpi = 300)
```


### estimate color in sRGB from wavelength

```{r}
require(colorscience)

## xyz to srgb
# XYZtoRGB: ColorSystem = c(xr, yr, xg, yg, xb, yb, xw, yw)
# for sRGB: c(0.64, 0.33, 0.3, 0.6, 0.15, 0.06, 0.31271, 0.32902)
# https://en.wikipedia.org/wiki/Color_spaces_with_RGB_primaries#Specifications
# https://en.wikipedia.org/wiki/Standard_illuminant#Illuminants_B_and_C
dw = seq(380, 700, by = 1)
ciexyz31 = ciexyz31 %>% as_tibble() ## color matching function 1931
ciexyz31 %>% gather(key = "key", value = "value", c(xbar, ybar, zbar)) %>% ggplot() + geom_line(aes(x = wlnm, y = value, color = key))

srgb = dw %>% colorscience::wlnm2xyz() %>% matrix(ncol = 3) %>% 
  as_tibble() %>% 
  bind_cols(dw = dw) %>% 
  group_by(dw) %>% 
  do({
    dat = .
    XYZtoRGB(xc = dat$V1[1], yc = dat$V2[1], zc = dat$V3[1], ColorSystem = c(0.64, 0.33, 0.3, 0.6, 0.15, 0.06, 0.31271, 0.32902)) %>% as_tibble()
    }) %>% 
  ungroup() %>% 
  filter(R >= 0 & R <= 1, G >= 0 & G <= 1, B >= 0 & B <= 1) %>% 
  mutate(srgb = rgb(R, G, B, maxColorValue = 1))
      

chromaticity.diagram.color.fill()
points(xyz[, 1], xyz[, 2], cex = 0.1)
```

## Modeling lake color to determine drivers

### import GLCP data

```{r}
## note: it takes 1hr to import the dataset

n = 1
j = 0
while(n != 0) {
  temp = read_csv(file = "~/Downloads/glcp.csv", col_types = "if----ff-nnn--n", n_max = 100000, skip = j * 100000, col_names = c("year", "Hylak_id", "bsn_lvl", "HYBAS_ID", "total_precip_mm", "mean_annual_temp_k", "pop_sum", "total_km2")) %>% 
    filter(year == 2015)
  n = nrow(temp)
  
  print(paste("index: ", j, "has n records: ", n))
  
  if (j==0) {
    glcp = temp
  } else {
    glcp = bind_rows(glcp, temp)
  }
  j = j + 1
}

save(glcp, file = "outputs/glcp_2015.RData")
```

### link lakeSamples to HydroLAKES

```{r}
require(sf)
lakeSampleDp = st_read("outputs/lakeSample_Dp.geojson")
hl = st_read(dsn = "~/Google_Drive/Map layers/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10_shp/HydroLAKES_polys_v10.shp")

lakeSampleHylakId = lakeSampleDp %>% 
  st_join(hl %>% select(Hylak_id, Res_time, Elevation, Depth_avg, Lake_area, Shore_dev, Lake_type), left = T) %>% 
  mutate(dp_lon = st_coordinates(geometry)[, 1], dp_lat = st_coordinates(geometry)[, 2]) %>% 
  st_drop_geometry() %>% 
  as_tibble()

print("How many lakes in lakeSampleDp?")
nrow(lakeSampleDp)
print("How many intersect with HydroLAKES?")
nrow(lakeSampleHylakId %>% filter(!is.na(Hylak_id)))

remove(lakeSampleDp, hl)
summary(lakeSampleHylakId)
#!!! we lost 30000 lakes when matching dp with hydroLakes, why?

save(lakeSampleHylakId, file = "outputs/lakeSampleHylakId.RData")
```

### match lake with their basin land cover percent

```{r}
## read in basin scale data

datBasin = dir("~/Google_Drive_morphologee1/Colors of lakes/", full.names = T) %>% 
  map(read_csv) %>% 
  reduce(rbind)
  
datBasin = datBasin %>% 
  transmute(
    HYBAS_ID = HYBAS_ID,
    total_area = total_area,
    lc_bare = bare_60 / total_area,
    cropland = cropland_40 / total_area,
    pwater = pwater_80 / total_area,
    snowIce = snowIce_70 / total_area,
    urban = urban_50 / total_area
  )
  
datBasin %>% summary()

save(datBasin, file = "outputs/basinLandCover_dd69dc1edce7dc8f026d0ac26d83389a.RData")
```

### match lakeSample id to hydroBasins and climate and population data and to color data

```{r}
load("outputs/glcp_2015.RData", verbose = T)

modelInput = lakeSampleHylakId %>% 
  select(-distance) %>% 
  na.omit() %>% 
  left_join(glcp %>% mutate(Hylak_id = as.integer(as.character(Hylak_id))) %>% select(-year, -bsn_lvl, -total_km2), by = "Hylak_id") %>%
  
  left_join(datFil, by = "id") %>% 
  select(-type) %>% 
  na.omit() %>% 
  mutate(HYBAS_ID = as.numeric(as.character(HYBAS_ID))) %>% 
  left_join(datBasin, by = "HYBAS_ID")

print("How many lakes not matched with hydrobasin?")
modelInput %>% filter(is.na(cropland)) %>% nrow()

modelInput = modelInput %>% na.omit 

save(modelInput, file = paste0("outputs/modelInput_07262020_", version, ".RData"))
```

### explore color vs depth, temperature, elevation, landcover

### quick cart model

```{r}
load(paste0("outputs/modelInput_07262020_", version, ".RData"), verbose = T)

require(rpart)
require(rpart.plot)
names(modelInput)
model = rpart(formula = mode_dw ~ Res_time + Elevation + depth + Shore_dev + class + total_precip_mm + mean_annual_temp_k + pop_sum + lc_bare + cropland + pwater + snowIce + urban, data = modelInput)

rpart.plot(model, nn = TRUE)

imp = model$variable.importance
imp = tibble(var = names(imp), importance = imp)
imp %>% ggplot() + geom_bar(aes(x = var, y = importance), orientation = "h", stat = "identity")


# library(party)
# fit = ctree(formula = color ~ Res_time + Elevation + depthMean + Shore_dev + class + total_precip_mm + mean_annual_temp_k + pop_sum, data = modelInput)
# plot(fit, main="Conditional Inference Tree for Lake color")
```

### quick model of unimodality

```{r}
require(rpart)
require(rpart.plot)

modelInputUnimodality = modelInput %>% 
  mutate(unimodal = factor(dw.pv >= 0.05, levels = c(T, F), labels = c("True", "False"))) %>% 
  select(unimodal, Res_time, Elevation, depth, Shore_dev, Lake_area, class, total_precip_mm, mean_annual_temp_k, pop_sum, lc_bare, cropland, pwater, snowIce, urban) %>% 
  filter(Res_time != -9999) %>% 
  na.omit

cart_model = rpart(formula = unimodal ~ Res_time + Elevation + depth + Shore_dev + class + total_precip_mm + mean_annual_temp_k + pop_sum + lc_bare + cropland + pwater + snowIce + urban, data = modelInputUnimodality)

rpart.plot(cart_model, nn = TRUE, type = 3)

imp = cart_model$variable.importance
imp = tibble(var = names(imp), importance = imp)
imp %>% ggplot() + geom_bar(aes(x = var, y = importance), orientation = "h", stat = "identity")



require(randomForest)

X = modelInputUnimodality[which(names(modelInputUnimodality) != "unimodal")]
Y = modelInputUnimodality$unimodal

# rf_fit_0 = tuneRF(X, Y, mtryStart = 3, ntreeTry = 50, stepFactor = 1, improve = 5, trace = T, plot = T, doBest = T, samplesize = 1000)

rf_fit_unimodal = randomForest(X, Y, samplesize = 1000, importance = TRUE, ntree = 100, do.trace = T)
# rf_fit_2 = randomForest(X, Y, samplesize = 1000, importance = TRUE, ntree = 100, do.trace = T)
# rf_fit_3 = randomForest(X, Y, samplesize = 2000, importance = TRUE, ntree = 150, do.trace = T)

save(rf_fit_unimodal, file = "outputs/unimodal_rf_models_08072020.RData")

rf_fit_unimodal
varImpPlot(rf_fit_unimodal)
```

## modal color drivers


### rf model of color groups
```{r}
load(paste0("outputs/modelInput_07262020_", version, ".RData"), verbose = T)

modelInputRf_colorGrp = modelInput %>% 
  mutate(colorGrp = cut(mode_dw, breaks = c(300, 525, 700), labels = c("Group 1", "Group 2"))) %>% 
  select(colorGrp, Res_time, Elevation, depth, Shore_dev, Lake_area, class, total_precip_mm, mean_annual_temp_k, pop_sum, lc_bare, cropland, pwater, snowIce, urban) %>% 
  filter(Res_time != -9999) %>% 
  na.omit
  
set.seed(2020)
splitIndex = runif(n = nrow(modelInputRf_colorGrp)) >= 0.2

training = modelInputRf_colorGrp[splitIndex, ]
validation = modelInputRf_colorGrp[!splitIndex, ]

X = training[which(names(training) != "colorGrp")]
Y = training$colorGrp

X_test = validation[which(names(validation) != "colorGrp")]
Y_test = validation$colorGrp

colorGrp_rf_fit = randomForest(X, Y, samplesize = 1000, importance = TRUE, ntree = 100, do.trace = T, xtest = X_test, ytest = Y_test, nodesize = 30, keep.forest = T)

save(colorGrp_rf_fit, file = "outputs/colorGrp_rf_models_08112020.RData")

varImpPlot(colorGrp_rf_fit)
```


### rf model of mode dw

```{r}
require(randomForest)

modelInputRf = modelInput %>% 
  select(mode_dw, Res_time, Elevation, depth, Shore_dev, Lake_area, class, total_precip_mm, mean_annual_temp_k, pop_sum, lc_bare, cropland, pwater, snowIce, urban) %>% 
  filter(Res_time != -9999) %>% 
  na.omit

require(corrplot)
corrplot(cor(modelInputRf %>% select(-class)))

set.seed(2020)
splitIndex = runif(n = nrow(modelInputRf)) >= 0.2

training = modelInputRf[splitIndex, ]
validation = modelInputRf[!splitIndex, ]

X = training[which(names(training) != "mode_dw")]
Y = training$mode_dw

X_test = validation[which(names(validation) != "mode_dw")]
Y_test = validation$mode_dw

dw_rf_fit = randomForest(X, Y, samplesize = 500, importance = TRUE, ntree = 100, do.trace = T, xtest = X_test, ytest = Y_test, nodesize = 30, keep.forest = T)

save(dw_rf_fit, file = "outputs/dw_rf_models_08062020.RData")

dw_rf_fit
varImpPlot(dw_rf_fit)

testData = validation %>% 
  select(mode_dw) %>% 
  bind_cols(tibble(pred = dw_rf_fit$test$predicted))

test_comp = testData %>% ggplot() +
  geom_hex(aes(x = mode_dw, y = pred, fill = ..count..)) + 
  geom_abline(aes(slope = 1, intercept = 0, color = "1:1")) +
  scale_y_continuous(limits = c(475, 575)) +
  scale_x_continuous(limits = c(475, 600)) +
  scale_fill_viridis_c() + labs(x = "Modal color observed (nm)", y = "Modal color predicted (nm)", color = "", fill = "Count")

test_comp

test_comp %>% ggsave(filename = "figs/dw_rf_test_comp.png", width = 6, height = 6)

testStats = testData %>% 
  mutate(dif = pred - mode_dw) %>% 
  summarise(
    mae = mean(abs(dif)),
    rmse = sqrt(mean(dif^2)),
    mbs = mean(dif)
  )

testStats
```

### Feature effect

```{r}
require(iml)
load("outputs/dw_rf_models_08062020.RData", verbose = T)
rf_fit = dw_rf_fit

predictor <- Predictor$new(rf_fit, data = X_test, y = Y_test)

imp <- FeatureImp$new(predictor, loss = "rmse", compare = "ratio", n.repetitions = 10)
plot(imp)
plot(imp) %>% ggsave(filename = "figs/dw_rf_importance.png", width = 7, height = 4)

depth_effs <- FeatureEffect$new(predictor, feature = "depth", method = "ale", grid.size = 50)
elevation_effs <- FeatureEffect$new(predictor, feature = "Elevation", method = "ale", grid.size = 50)
temp_effs <- FeatureEffect$new(predictor, feature = "mean_annual_temp_k", method = "ale", grid.size = 50)
cropland_effs <- FeatureEffect$new(predictor, feature = "cropland", method = "ale", grid.size = 50)
shoreDev_effs <- FeatureEffect$new(predictor, feature = "Shore_dev", method = "ale", grid.size = 50)
pop_effs <- FeatureEffect$new(predictor, feature = "pop_sum", method = "ale", grid.size = 50)
area_effs <- FeatureEffect$new(predictor, feature = "Lake_area", method = "ale", grid.size = 50)
pwater_effs <- FeatureEffect$new(predictor, feature = "pwater", method = "ale", grid.size = 50)
Res_time_effs <- FeatureEffect$new(predictor, feature = "Res_time", method = "ale", grid.size = 25)


p1 = depth_effs$plot() + geom_point()
p2 = elevation_effs$plot() + geom_point()
p3 = temp_effs$plot() + geom_point()
p4 = shoreDev_effs$plot() + geom_point()
p5 = cropland_effs$plot() + geom_point()
p6 = pwater_effs$plot() + geom_point()
p7 = area_effs$plot() + geom_point()
# p8 = Res_time_effs$plot() + geom_point() + scale_x_log10()


require(patchwork)

featureEffectALE = p1 + p2 + p3 + p4 + p5 + p6 + p7 + plot_layout(ncol = 4, byrow = T) +
  plot_annotation(title = "Feature effect: ALE")

featureEffectALE

featureEffectALE %>% ggsave(filename = "figs/dw_featureEffectALE.png", width = 10, height = 5)


# pop_effs$plot() + geom_point() + scale_x_log10()
```

### dw binned by important features (not relying on model)

```{r}
quantileGrp = function(x) {
  return(cut(x, breaks = unique(quantile(x, probs = seq(0, 1, by = 0.1))), include.lowest = T, ordered_result = T))
}

quantileGrpZeros = function(x) {
  output = x
  index0 = which(x == 0)
  indexNone0 = which(x != 0)
  output[index0] = "0"
  output[indexNone0] = as.character(cut(x[indexNone0], breaks = unique(quantile(x[indexNone0], probs = seq(0, 1, length.out = 10))), include.lowest = T, ordered_result = T))
  level = cut(x[indexNone0], breaks = unique(quantile(x[indexNone0], probs = seq(0, 1, length.out = 10))), include.lowest = T, ordered_result = T) %>% levels()
  return(factor(output, levels = c("0", level), labels = c("0", level), ordered = T))
}

modelInputGrped = modelInputRf %>% 
  mutate(depth_grp = quantileGrp(depth),
         Elevation_grp = quantileGrp(Elevation),
         mean_annual_temp_k_grp = quantileGrp(mean_annual_temp_k),
         Shore_dev_grp = quantileGrp(Shore_dev),
         pop_sum_grp = quantileGrpZeros(pop_sum),
         cropland_grp = quantileGrpZeros(cropland),
         pwater_grp = quantileGrp(pwater),
         total_precip_mm_grp = quantileGrp(total_precip_mm),
         Lake_area_grp = quantileGrp(Lake_area),
         Res_time_grp = quantileGrp(Res_time),
         lc_bare_grp = quantileGrpZeros(lc_bare),
         urban_grp = quantileGrpZeros(urban))

plotViolin = function(grp, data = modelInputGrped, value = mode_dw) {
  data %>% 
  ggplot(aes(x = {{grp}}, y = {{value}})) +
    geom_violin(trim = T, scale = "width", fill = "lightgrey") +
    stat_summary(fun = mean, geom = "point", size = 2, color = "blue") +
    stat_summary(fun = median, geom = "point", size = 2, color = "red") +
    labs(y = "Modal dw (nm)", title = substitute({{grp}})) +
    scale_y_continuous(limits = c(450, 600)) +
    theme(axis.text.x.bottom = element_text(angle = -90, vjust = 0.5, hjust = 0),
          axis.title.x.bottom = element_blank())
}

plotViolin(depth_grp)

p1 = plotViolin(depth_grp)
p2 = plotViolin(Elevation_grp)
p3 = plotViolin(mean_annual_temp_k_grp)
p4 = plotViolin(Shore_dev_grp)
p5 = plotViolin(pop_sum_grp)
p6 = plotViolin(cropland_grp)
p7 = plotViolin(pwater_grp)
p8 = plotViolin(total_precip_mm_grp)
p9 = plotViolin(Lake_area_grp)
p10 = plotViolin(Res_time_grp)
p11 = plotViolin(lc_bare_grp)
p12 = plotViolin(urban_grp)

require(patchwork)

featureEffectViolin = p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + plot_layout(ncol = 6, byrow = T)
# + plot_annotation(title = "Feature effect: quantile groups")

featureEffectViolin

featureEffectViolin %>% ggsave(filename = "figs/dw_featureEffectViolin.png", width = 12, height = 7)



modelInputGrped %>%
  ggplot(aes(x = lc_bare_grp, y = mode_dw)) +
  geom_violin(trim = T, scale = "width", fill = "lightgrey") +
  stat_summary(fun = mean, geom = "point", size = 2, color = "blue") +
  stat_summary(fun = median, geom = "point", size = 2, color = "red") +
  labs(y = "Modal dw (nm)") +
  theme(axis.text.x.bottom = element_text(angle = -90, vjust = 0.5, hjust = 0))
```



## color variability drivers

### rf model of color variation

```{r}
require(randomForest)

modelInputRfdwstd = modelInput %>% 
  select(std_dw, Res_time, Elevation, depth, Shore_dev, Lake_area, class, total_precip_mm, mean_annual_temp_k, pop_sum, lc_bare, cropland, pwater, snowIce, urban) %>% 
  filter(Res_time != -9999) %>% 
  na.omit

require(corrplot)
corrplot(cor(modelInputRfdwstd %>% select(-class)))

set.seed(2020)
splitIndex = runif(n = nrow(modelInputRfdwstd)) >= 0.2

training = modelInputRfdwstd[splitIndex, ]
validation = modelInputRfdwstd[!splitIndex, ]

X = training[which(names(training) != "std_dw")]
Y = training$std_dw

X_test = validation[which(names(validation) != "std_dw")]
Y_test = validation$std_dw

std_rf_fit = randomForest(X, Y, samplesize = 500, importance = TRUE, ntree = 100, do.trace = T, xtest = X_test, ytest = Y_test, nodesize = 30, keep.forest = T)

save(std_rf_fit, file = "outputs/dw_std_rf_models_08062020.RData")

std_rf_fit
varImpPlot(std_rf_fit)

testData = validation %>% 
  select(std_dw) %>% 
  bind_cols(tibble(pred = std_rf_fit$test$predicted))

test_comp = testData %>% ggplot() +
  geom_hex(aes(x = std_dw, y = pred, fill = ..count..)) + 
  geom_abline(aes(slope = 1, intercept = 0, color = "1:1")) +
  scale_fill_viridis_c() + labs(x = "std Modal color observed (nm)", y = "std Modal color predicted (nm)", color = "", fill = "Count")

test_comp

test_comp %>% ggsave(filename = "figs/dw_std_rf_test_comp.png", width = 6, height = 6)

testStats = testData %>% 
  mutate(dif = pred - std_dw) %>% 
  summarise(
    mae = mean(abs(dif)),
    rmse = sqrt(mean(dif^2)),
    mbs = mean(dif)
  )

testStats
```

### Feature effect

```{r}
require(iml)
load("outputs/dw_std_rf_models_08062020.RData", verbose = T)
rf_fit = std_rf_fit

predictor <- Predictor$new(rf_fit, data = X_test, y = Y_test)

imp <- FeatureImp$new(predictor, loss = "rmse", compare = "ratio", n.repetitions = 1)
plot(imp)
plot(imp) %>% ggsave(filename = "figs/dw_std_rf_importance.png", width = 7, height = 4)

depth_effs <- FeatureEffect$new(predictor, feature = "depth", method = "ale", grid.size = 50)
elevation_effs <- FeatureEffect$new(predictor, feature = "Elevation", method = "ale", grid.size = 50)
temp_effs <- FeatureEffect$new(predictor, feature = "mean_annual_temp_k", method = "ale", grid.size = 50)
cropland_effs <- FeatureEffect$new(predictor, feature = "cropland", method = "ale", grid.size = 50)
shoreDev_effs <- FeatureEffect$new(predictor, feature = "Shore_dev", method = "ale", grid.size = 50)
pop_effs <- FeatureEffect$new(predictor, feature = "pop_sum", method = "ale", grid.size = 50)
area_effs <- FeatureEffect$new(predictor, feature = "Lake_area", method = "ale", grid.size = 50)
pwater_effs <- FeatureEffect$new(predictor, feature = "pwater", method = "ale", grid.size = 50)
Res_time_effs <- FeatureEffect$new(predictor, feature = "Res_time", method = "ale", grid.size = 25)


p1 = depth_effs$plot() + geom_point()
p2 = elevation_effs$plot() + geom_point()
p3 = temp_effs$plot() + geom_point()
p4 = shoreDev_effs$plot() + geom_point()
p5 = cropland_effs$plot() + geom_point()
p6 = pwater_effs$plot() + geom_point()
p7 = area_effs$plot() + geom_point()
# p8 = Res_time_effs$plot() + geom_point() + scale_x_log10()


require(patchwork)

featureEffectALE = p1 + p2 + p3 + p4 + p5 + p6 + p7 + plot_layout(ncol = 4, byrow = T) +
  plot_annotation(title = "Feature effect: ALE")

featureEffectALE

featureEffectALE %>% ggsave(filename = "figs/dw_std_featureEffectALE.png", width = 10, height = 5)


# pop_effs$plot() + geom_point() + scale_x_log10()
```

### dw std binned by important features (not relying on model)

```{r}
quantileGrp = function(x) {
  return(cut(x, breaks = unique(quantile(x, probs = seq(0, 1, by = 0.1))), include.lowest = T, ordered_result = T))
}

quantileGrpZeros = function(x) {
  output = x
  index0 = which(x == 0)
  indexNone0 = which(x != 0)
  output[index0] = "0"
  output[indexNone0] = as.character(cut(x[indexNone0], breaks = unique(quantile(x[indexNone0], probs = seq(0, 1, length.out = 10))), include.lowest = T, ordered_result = T))
  level = cut(x[indexNone0], breaks = unique(quantile(x[indexNone0], probs = seq(0, 1, length.out = 10))), include.lowest = T, ordered_result = T) %>% levels()
  return(factor(output, levels = c("0", level), labels = c("0", level), ordered = T))
}

modelInputGrped = modelInputRfdwstd %>% 
  mutate(depth_grp = quantileGrp(depth),
         Elevation_grp = quantileGrp(Elevation),
         mean_annual_temp_k_grp = quantileGrp(mean_annual_temp_k),
         Shore_dev_grp = quantileGrp(Shore_dev),
         pop_sum_grp = quantileGrpZeros(pop_sum),
         cropland_grp = quantileGrpZeros(cropland),
         pwater_grp = quantileGrp(pwater),
         total_precip_mm_grp = quantileGrp(total_precip_mm),
         Lake_area_grp = quantileGrp(Lake_area),
         Res_time_grp = quantileGrp(Res_time),
         lc_bare_grp = quantileGrpZeros(lc_bare),
         urban_grp = quantileGrpZeros(urban))

plotViolin = function(grp, data = modelInputGrped, value = std_dw) {
  data %>% 
  ggplot(aes(x = {{grp}}, y = {{value}})) +
    geom_violin(trim = T, scale = "width", fill = "lightgrey") +
    stat_summary(fun = mean, geom = "point", size = 2, color = "blue") +
    stat_summary(fun = median, geom = "point", size = 2, color = "red") +
    labs(y = "std dw (nm)", title = substitute({{grp}})) +
    theme(axis.text.x.bottom = element_text(angle = -90, vjust = 0.5, hjust = 0),
          axis.title.x.bottom = element_blank())
}

plotViolin(mean_annual_temp_k_grp)

p5 = plotViolin(depth_grp)
p3 = plotViolin(Elevation_grp)
p1 = plotViolin(mean_annual_temp_k_grp)
p2 = plotViolin(pop_sum_grp)
p4 = plotViolin(cropland_grp)
p8 = plotViolin(pwater_grp)
p6 = plotViolin(total_precip_mm_grp)
p9 = plotViolin(Res_time_grp)
p7 = plotViolin(lc_bare_grp)

require(patchwork)

featureEffectViolin_stw = p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + plot_layout(ncol = 6, byrow = T)
# + plot_annotation(title = "Feature effect: quantile groups")

featureEffectViolin_stw

featureEffectViolin_stw %>% ggsave(filename = "figs/dw_std_featureEffectViolin.png", width = 12, height = 7)



modelInputGrped %>%
  ggplot(aes(x = lc_bare_grp, y = mode_dw)) +
  geom_violin(trim = T, scale = "width", fill = "lightgrey") +
  stat_summary(fun = mean, geom = "point", size = 2, color = "blue") +
  stat_summary(fun = median, geom = "point", size = 2, color = "red") +
  labs(y = "Modal dw (nm)") +
  theme(axis.text.x.bottom = element_text(angle = -90, vjust = 0.5, hjust = 0))
```





## approx rgb from wavelength for visualization in GEE

```{r}
# method1
colorscience::heuristic.wlnm2RGB(wavelength = seq(380, 700, length.out = 100)) %>% as_tibble() %>% 
  mutate(color_rgb = as.character(rgb(red = R, green = G, blue = B, maxColorValue = 1))) %>% 
  pull(color_rgb) %>% 
  paste0(collapse = "', '")

#method2
dw2rgb = function(w) {
  if (w >= 380 & w < 440) {
    R = -(w - 440.) / (440. - 380.)
    G = 0.0
    B = 1.0
  } else if (w >= 440 & w < 490) {
    R = 0.0
    G = (w - 440.) / (490. - 440.)
    B = 1.0
  } else if (w >= 490 & w < 510) {
    R = 0.0
    G = 1.0
    B = -(w - 510.) / (510. - 490.)
  } else if (w >= 510 & w < 580) {
    R = (w - 510.) / (580. - 510.)
    G = 1.0
    B = 0.0
  } else if (w >= 580 & w < 645) {
    R = 1.0
    G = -(w - 645.) / (645. - 580.)
    B = 0.0
  } else if (w >= 645 & w <= 780) {
    R = 1.0
    G = 0.0
    B = 0.0
  } else {
    R = 0.0
    G = 0.0
    B = 0.0
  }
        
  # return(rgb(R, G, B, maxColorValue = 1))
  return(tibble(R, G, B, rgb = rgb(R, G, B, maxColorValue = 1)))
}

tibble(w = seq(380, 700, length.out = 100)) %>% 
  group_by(w) %>% 
  do({dw2rgb(.$w[1])}) %>% 
  ungroup() %>% 
  pull(rgb) %>% 
  paste0(collapse = "', '")
## implement this in GEE lookup table method
# https://code.earthengine.google.com/c44243894ad2c9b0f891e4adee121e62

## method 3 modify the existing function
wl = 380:700
test_color = tibble(wl = wl) %>% 
  bind_cols(heuristic.wlnm2RGB_modified(wl, Gamma = 0.8, IntensityMax = 1))

ggplot(data = test_color) + geom_point(aes(x = wl, y = 1, color = wl)) +
  scale_color_gradientn(colors = test_color$color, limits = c(380, 700))
```

